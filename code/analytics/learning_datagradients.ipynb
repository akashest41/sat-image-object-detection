{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVWhXLGM7MEUMzgceRtnKt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":72,"metadata":{"id":"UEC1XBI-Jpnf","executionInfo":{"status":"ok","timestamp":1708953576843,"user_tz":0,"elapsed":571,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"outputs":[],"source":["# Mounting your Google Drive\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Switching to the directory of this notebook\n","\n","# %cd /content/drive/MyDrive/EYOpenScienceDataChallenge/data/data_gradients_practice"],"metadata":{"id":"K4FxcxUYJu-6","executionInfo":{"status":"ok","timestamp":1708953599345,"user_tz":0,"elapsed":469,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["# Ensuring the above process worked as expected\n","\n","# !pwd"],"metadata":{"id":"Hi3rfBS5J29d","executionInfo":{"status":"ok","timestamp":1708953607706,"user_tz":0,"elapsed":207,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# install data gradients\n","\n","# ! pip install data-gradients"],"metadata":{"id":"-Qnns3WyKNAV","executionInfo":{"status":"ok","timestamp":1708953616705,"user_tz":0,"elapsed":202,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["# Download and extract your training set. There should be a single folder with all images and labelme style annotation jsons.\n","\n","# !tar -xvf ./training_data_3.tar"],"metadata":{"id":"4Bv4UVBxKiY4","executionInfo":{"status":"ok","timestamp":1708953656902,"user_tz":0,"elapsed":353,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["# install labelme2coco; this will prepare the annotations in the COCO format, which is what datagradients deals with\n","\n","# !pip install -U labelme2coco"],"metadata":{"id":"Gc9Zud4CLGqV","executionInfo":{"status":"ok","timestamp":1708953683606,"user_tz":0,"elapsed":291,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["# run labelme2coco\n","\n","import labelme2coco\n","\n","# set directory that contains labelme annotations and image files\n","labelme_folder = \"./training_data_3\"\n","\n","# set export dir\n","export_dir = \"./train_coco_format\"\n","\n","# set train split rate\n","train_split_rate = 0.85\n","\n","# labelme2coco.convert(labelme_folder, export_dir, train_split_rate)"],"metadata":{"id":"hT35zAwVMG2P","executionInfo":{"status":"ok","timestamp":1708953728622,"user_tz":0,"elapsed":212,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["# loading data gradients dependencies\n","\n","from data_gradients.managers.detection_manager import DetectionAnalysisManager\n","from data_gradients.datasets.detection.coco_detection_dataset import COCODetectionDataset"],"metadata":{"id":"x5Rg6Z49Ml8C","executionInfo":{"status":"ok","timestamp":1708953752395,"user_tz":0,"elapsed":242,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["# reading in training and validation set annotations\n","\n","train_set = COCODetectionDataset(root_dir=\"./train_coco_format/\", split=\"train\", year=\"\")\n","val_set = COCODetectionDataset(root_dir=\"./train_coco_format/\", split=\"val\", year=\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zDJB18oPPT2a","executionInfo":{"status":"ok","timestamp":1708953771164,"user_tz":0,"elapsed":253,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}},"outputId":"574fb2e7-611f-4f8b-a1ae-47f52ba78cb0"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.02s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n"]}]},{"cell_type":"code","source":["# preparing the analytics manager object\n","\n","manager = DetectionAnalysisManager(\n","    report_title=\"Training Data 3 Draft\",\n","    train_data=train_set,\n","    val_data=val_set,\n","    class_names=train_set.class_names,\n",")"],"metadata":{"id":"HjpofQvWP1PT","executionInfo":{"status":"ok","timestamp":1708953783744,"user_tz":0,"elapsed":693,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["# labelme2coco only prepares the annotations\n","\n","# to use data gradients, we need to also prepare the directory structure generated by labelme2coco\n","# in the right format, containing all the images, to be read in by the datagradients object\n","\n","import json\n","import shutil\n","import os\n","\n","with open('./train_coco_format/annotations/instances_train.json', 'r') as file:\n","    data = json.load(file)\n","train_files = []\n","for filedata in data[\"images\"]:\n","  train_files.append(filedata['file_name'])\n","\n","with open('./train_coco_format/annotations/instances_val.json', 'r') as file:\n","    data = json.load(file)\n","val_files = []\n","for filedata in data[\"images\"]:\n","  val_files.append(filedata['file_name'])\n","\n","source_dir = './training_data_3/'\n","train_dest_dir = './train_coco_format/images/train'\n","if not os.path.exists(train_dest_dir):\n","  os.makedirs(train_dest_dir)\n","val_dest_dir = './train_coco_format/images/val'\n","if not os.path.exists(val_dest_dir):\n","  os.makedirs(val_dest_dir)\n","\n","for file in train_files:\n","  source_path = os.path.join(source_dir, file)\n","  destination_path = os.path.join(train_dest_dir, file)\n","  shutil.copy2(source_path, destination_path)\n","\n","for file in val_files:\n","  source_path = os.path.join(source_dir, file)\n","  destination_path = os.path.join(val_dest_dir, file)\n","  shutil.copy2(source_path, destination_path)"],"metadata":{"id":"cBF72-MgSBFl","executionInfo":{"status":"ok","timestamp":1708953857167,"user_tz":0,"elapsed":1189,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["# that's it - run the analytics manager!\n","# it's interactive\n","\n","manager.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7u4-Ei2RRmkf","outputId":"64218cd5-faf7-4252-8f9b-6256a3d29758"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  - Executing analysis with: \n","  - batches_early_stop: None \n","  - len(train_data): 39 \n","  - len(val_data): 8 \n","  - log directory: /content/drive/MyDrive/EYOpenScienceDataChallenge/data/data_gradients_practice/logs/Training_Data_3_Draft \n","  - Archive directory: /content/drive/MyDrive/EYOpenScienceDataChallenge/data/data_gradients_practice/logs/Training_Data_3_Draft/archive_20240226-132302 \n","  - feature extractor list: {'Image Features': [SummaryStats, ImagesResolution, ImageColorDistribution, ImagesAverageBrightness], 'Object Detection Features': [DetectionClassHeatmap, DetectionBoundingBoxArea, DetectionBoundingBoxPerImageCount, DetectionBoundingBoxSize, DetectionClassFrequency, DetectionClassesPerImageCount, DetectionBoundingBoxIoU, DetectionResizeImpact]}\n","\u001b[34;1m╔\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m╗\u001b[0m\n","\u001b[34;1m║  \u001b[0mTo better understand how to tackle the data issues highlighted in this\u001b[34;1m  ║\u001b[0m\n","\u001b[34;1m║  \u001b[0mreport, explore our comprehensive course on analyzing computer vision \u001b[34;1m  ║\u001b[0m\n","\u001b[34;1m║  \u001b[0mdatasets. click here: https://hubs.ly/Q01XpHBT0                       \u001b[34;1m  ║\u001b[0m\n","\u001b[34;1m╚\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m═\u001b[0m\u001b[34;1m╝\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["\rAnalyzing... :   0%|          | 0/39 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","--------------------------------------------------------------------------------\n","\u001b[33;1mIn which format are your images loaded ?\u001b[0m\n","--------------------------------------------------------------------------------\n","\n","\u001b[34;1mOptions\u001b[0m:\n","[\u001b[34;1m0\u001b[0m] | RGB\n","[\u001b[34;1m1\u001b[0m] | BGR\n","[\u001b[34;1m2\u001b[0m] | LAB\n","[\u001b[34;1m3\u001b[0m] | Other\n","\n","Your selection (Enter the \u001b[34;1mcorresponding number\u001b[0m) >>> 0\n","Great! \u001b[33;1mYou chose: `RGB`\u001b[0m\n","\n","--------------------------------------------------------------------------------\n","\u001b[33;1m\u001b[33;1mWhich comes first\u001b[0m in your annotations, the class id or the bounding box?\u001b[0m\n","--------------------------------------------------------------------------------\n","Here's a sample of how your labels look like:\n","Each line corresponds to a bounding box.\n","tensor([[  2.0000, 421.4167, 254.7500,  67.5000,  89.1667],\n","        [  0.0000, 320.5833, 307.2500,  93.3333, 103.3333],\n","        [  2.0000, 369.7500, 388.0833,  57.5000,  54.1667]])\n","\u001b[34;1mOptions\u001b[0m:\n","[\u001b[34;1m0\u001b[0m] | Label comes first (e.g. [class_id, x1, y1, x2, y2])\n","[\u001b[34;1m1\u001b[0m] | Bounding box comes first (e.g. [x1, y1, x2, y2, class_id])\n","\n","Your selection (Enter the \u001b[34;1mcorresponding number\u001b[0m) >>> 0\n","Great! \u001b[33;1mYou chose: `Label comes first (e.g. [class_id, x1, y1, x2, y2])`\u001b[0m\n","\n","--------------------------------------------------------------------------------\n","\u001b[33;1mWhat is the \u001b[33;1mbounding box format\u001b[0m?\u001b[0m\n","--------------------------------------------------------------------------------\n","Here's a sample of how your labels look like:\n","Each line corresponds to a bounding box.\n","tensor([[  2.0000, 421.4167, 254.7500,  67.5000,  89.1667],\n","        [  0.0000, 320.5833, 307.2500,  93.3333, 103.3333],\n","        [  2.0000, 369.7500, 388.0833,  57.5000,  54.1667]])\n","\u001b[34;1mOptions\u001b[0m:\n","[\u001b[34;1m0\u001b[0m] | xyxy: x-left, y-top, x-right, y-bottom\t\t(Pascal-VOC format)\n","[\u001b[34;1m1\u001b[0m] | xywh: x-left, y-top, width, height\t\t\t(COCO format)\n","[\u001b[34;1m2\u001b[0m] | cxcywh: x-center, y-center, width, height\t\t(YOLO format)\n"]}]},{"cell_type":"code","source":["!rm -rf logs"],"metadata":{"id":"kMAMpwaxRpBb","executionInfo":{"status":"ok","timestamp":1708953548137,"user_tz":0,"elapsed":231,"user":{"displayName":"Akash Bajaj","userId":"14730281138542257808"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9IRfMPDMjAnD"},"execution_count":null,"outputs":[]}]}